# AI Specialist & Data Engineer Assessment Project
## YouTube Video Title Classification Pipeline

### Project Overview
This project demonstrates a complete end-to-end pipeline for scraping YouTube video titles, classifying them using AI, and storing the results in a database. The system showcases skills in web scraping, data classification, data modeling, and scalable architecture design.

### 1. Architecture & System Design

#### Key Components:
- **ScraperEngine**: Orchestrates the scraping, classification, and storage pipeline
- **HeadlineClassifier**: Zero-shot AI classification using HuggingFace transformers
- **Database**: SQLite with connection pooling and batch processing
- **Playwright**: Web scraping engine for dynamic content
- **Configuration System**: JSON-based configuration for flexibility

#### Flow Structure:
```
YouTube Channels → Web Scraping → AI Classification → Database Storage → Analytics
```

#### Scalability Considerations:
- **Connection Pooling**: Database connections are pooled for efficient resource management
- **Batch Processing**: Database operations are batched for performance optimization
- **Modular Design**: Components are encapsulated for easy scaling and testing
- **Configuration-Driven**: Easy to add new sources and categories without code changes

### 2. Web Scraping Approach

#### Tools & Technology:
- **Playwright**: Chosen for its ability to handle dynamic JavaScript content
- **CSS Selectors**: Used for extracting video titles from YouTube pages
- **Error Handling**: Robust exception handling for network issues and site changes

#### Dynamic Content Handling:
- **Wait Strategies**: Uses `wait_until="domcontentloaded"` for proper page loading
- **Selector Fallbacks**: Multiple selector strategies for reliability
- **Timeout Management**: Configurable timeouts for different network conditions

#### Limitations & Solutions:
- **Rate Limiting**: Implemented delays and error handling
- **Site Changes**: Flexible selector system to adapt to UI changes
- **Anti-Bot Measures**: Uses headless browser with realistic user agent

### 3. Data Classification Strategy

#### AI Tools & Approach:
- **HuggingFace Zero-Shot Classification**: Using `facebook/bart-large-mnli` model
- **Custom Categories**: Configurable classification labels for marketing/business content
- **Confidence Thresholds**: Quality control through confidence scoring

#### Prompt Engineering & Efficiency:
- **Zero-Shot Approach**: No training required, immediate deployment
- **Batch Processing**: Efficient processing of multiple titles
- **Cost Optimization**: Local model execution, no API costs
- **Context Management**: Structured input/output for consistent results

#### Classification Categories:
- Marketing Strategy
- Business Tips
- AI/Technology
- Social Media
- SEO/Digital Marketing
- Business Growth
- Tutorial/How-to

### 4. Data Modeling & Storage

#### Database Choice:
- **SQLite**: Chosen for simplicity, portability, and ACID compliance
- **Rationale**: Perfect for prototyping and small to medium-scale deployments
- **Scalability Path**: Can be easily migrated to PostgreSQL or cloud databases

#### Data Schema:
```sql
CREATE TABLE headlines (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source TEXT NOT NULL,           -- YouTube channel/source name
    headline TEXT NOT NULL,         -- Video title
    category TEXT NOT NULL,         -- AI classification result
    raw_label TEXT,                 -- Raw model output
    confidence REAL,                -- Classification confidence score
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### Data Model Benefits:
- **Audit Trail**: Timestamp tracking for all operations
- **Quality Metrics**: Confidence scores for result validation
- **Source Tracking**: Multiple source support
- **Flexibility**: Raw label storage for debugging and analysis

### 5. Technical Implementation Highlights

#### Performance Optimizations:
- **Batch Processing**: 50-record batches for database operations
- **Connection Pooling**: 3-connection pool for database efficiency
- **Memory Management**: Automatic batch flushing and cleanup

#### Error Handling & Reliability:
- **Graceful Degradation**: Continues processing even if individual items fail
- **Data Validation**: Input validation for all database operations
- **Resource Management**: Proper cleanup of database connections

#### Code Quality:
- **Type Hints**: Full type annotation for maintainability
- **Documentation**: Comprehensive docstrings and comments
- **Modular Design**: Separation of concerns for testability

### 6. Scalability & Future Improvements

#### Current Scalability:
- **Horizontal Scaling**: Can run multiple instances for different sources
- **Database Scaling**: Connection pooling supports concurrent operations
- **Configuration Scaling**: Easy to add new sources and categories

#### Long-term Improvements:
- **Message Queues**: Implement RabbitMQ/SQS for distributed processing
- **Cloud Database**: Migrate to PostgreSQL or cloud-native solutions
- **Microservices**: Split into separate scraping, classification, and storage services
- **Monitoring**: Add metrics collection and alerting systems
- **Caching**: Implement Redis for frequently accessed data
- **API Layer**: REST API for data access and management
